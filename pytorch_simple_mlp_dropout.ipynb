{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this notebook ,\n",
    "I add dropout.\n",
    "When p =0.5 for dropout , nonzero outputs are doubled\n",
    "1 / 1-p = 1 / 0.5 = 2\n",
    "\n",
    "if p = 0.2\n",
    "1 / 1-0.2 = 1 / 0.8 = 1.25\n",
    "\"\"\"\n",
    "import torch \n",
    "import numpy \n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_ = 1\n",
    "\n",
    "def dump_info(tensor_,title):\n",
    "    if dump_ == 1:\n",
    "        print(title)\n",
    "        print( list(tensor_.size() ))\n",
    "        print(tensor_)\n",
    "        \n",
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size,drop_out_prob):\n",
    "            print(\"Feedforward input_size \",input_size,\" hidden_size \", hidden_size,\" drop_out_prob\",drop_out_prob)\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.drop = torch.nn.Dropout(p=drop_out_prob)\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()        \n",
    "        \n",
    "        def forward(self, x):\n",
    "            global dump_\n",
    "            \n",
    "            dump_info(x,\"input\")\n",
    "            \n",
    "            hidden = self.fc1(x)            \n",
    "            dump_info(hidden,\"hidden fc1\")\n",
    "            \n",
    "            #drop_output =  torch.nn.functional.dropout(hidden) \n",
    "            drop_output =  self.drop(hidden) \n",
    "            dump_info(drop_output,\"drop_output\")\n",
    "            \n",
    "            relu = self.relu(drop_output)            \n",
    "            dump_info(relu,\"relu\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            output = self.fc2(relu)            \n",
    "            dump_info(output,\"output fc2\")\n",
    "            \n",
    "            sigmoid_output = self.sigmoid(output)\n",
    "            dump_info(sigmoid_output,\"sigmoid_output\")\n",
    "            #close logging\n",
    "            dump_ = 2\n",
    "            \n",
    "            return sigmoid_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE RANDOM DATA POINTS\n",
    "total_samples = 5\n",
    "x_train, y_train = make_classification(n_samples=total_samples,n_features=2, n_redundant=0, n_informative=1,\n",
    "                             n_clusters_per_class=1)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "x_test, y_test = make_classification(n_samples=total_samples,n_features=2, n_redundant=0, n_informative=1,\n",
    "                             n_clusters_per_class=1)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward input_size  2  hidden_size  4  drop_out_prob 0.5\n",
      "Feedforward(\n",
      "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (drop): Dropout(p=0.5)\n",
      "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "input\n",
      "[5, 2]\n",
      "tensor([[ 0.2435,  1.1330],\n",
      "        [-1.1941, -0.3866],\n",
      "        [ 1.5834,  0.1661],\n",
      "        [-1.2566, -0.0566],\n",
      "        [-1.0901, -1.0576]])\n",
      "hidden fc1\n",
      "[5, 4]\n",
      "tensor([[-0.2873, -0.2479,  0.9685,  1.0959],\n",
      "        [ 0.2281, -0.7970, -0.6134,  0.0622],\n",
      "        [-0.7914, -0.1276,  1.3661,  0.8261],\n",
      "        [ 0.2544, -0.7558, -0.5032,  0.2223],\n",
      "        [ 0.1830, -0.8855, -0.8518, -0.2670]], grad_fn=<AddmmBackward>)\n",
      "drop_output\n",
      "[5, 4]\n",
      "tensor([[-0.0000, -0.0000,  0.0000,  2.1918],\n",
      "        [ 0.0000, -0.0000, -1.2268,  0.1245],\n",
      "        [-0.0000, -0.2553,  2.7322,  1.6523],\n",
      "        [ 0.5088, -1.5116, -1.0064,  0.4445],\n",
      "        [ 0.0000, -0.0000, -1.7036, -0.0000]], grad_fn=<MulBackward0>)\n",
      "relu\n",
      "[5, 4]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 2.1918],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1245],\n",
      "        [0.0000, 0.0000, 2.7322, 1.6523],\n",
      "        [0.5088, 0.0000, 0.0000, 0.4445],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "output fc2\n",
      "[5, 1]\n",
      "tensor([[ 0.6481],\n",
      "        [ 0.4760],\n",
      "        [-0.3655],\n",
      "        [ 0.7136],\n",
      "        [ 0.4656]], grad_fn=<AddmmBackward>)\n",
      "sigmoid_output\n",
      "[5, 1]\n",
      "tensor([[0.6566],\n",
      "        [0.6168],\n",
      "        [0.4096],\n",
      "        [0.6712],\n",
      "        [0.6144]], grad_fn=<SigmoidBackward>)\n",
      "Test loss before training 0.8675163984298706\n"
     ]
    }
   ],
   "source": [
    "model = Feedforward(2, 4,0.5)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "print(model)\n",
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())\n",
    "\n",
    "#input        [5, 2]  our input shape\n",
    "#hidden fc1   [5, 4]  (fc1): Linear(in_features=2, out_features=4, bias=True) \n",
    "#drop_output  [5, 4]  since =0.5 nonzero outputs are doubled\n",
    "#relu         [5, 4]  relu same size\n",
    "#output fc2   [5, 1]  Linear(in_features=4, out_features=1, bias=True)\n",
    "#sigmoid_output  [5, 1]  Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.7601617574691772\n",
      "Epoch 1: train loss: 0.8074312210083008\n",
      "Epoch 2: train loss: 0.7312209010124207\n",
      "Epoch 3: train loss: 0.7304747104644775\n",
      "Epoch 4: train loss: 0.6818045377731323\n",
      "Epoch 5: train loss: 0.8593791127204895\n",
      "Epoch 6: train loss: 0.8514102101325989\n",
      "Epoch 7: train loss: 0.774050235748291\n",
      "Epoch 8: train loss: 0.7576993703842163\n",
      "Epoch 9: train loss: 0.7600385546684265\n",
      "Epoch 10: train loss: 0.6953762769699097\n",
      "Epoch 11: train loss: 0.719214141368866\n",
      "Epoch 12: train loss: 0.7432788014411926\n",
      "Epoch 13: train loss: 0.696179211139679\n",
      "Epoch 14: train loss: 0.7173563241958618\n",
      "Epoch 15: train loss: 0.730347752571106\n",
      "Epoch 16: train loss: 0.6704256534576416\n",
      "Epoch 17: train loss: 0.7253202199935913\n",
      "Epoch 18: train loss: 0.7618869543075562\n",
      "Epoch 19: train loss: 0.8282432556152344\n",
      "Test loss after Training 0.9036760330200195\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):    \n",
    "    optimizer.zero_grad()    \n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)    \n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test) \n",
    "print('Test loss after Training' , after_train.item())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward input_size  2  hidden_size  4  drop_out_prob 0.2\n",
      "Feedforward(\n",
      "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (drop): Dropout(p=0.2)\n",
      "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "input\n",
      "[5, 2]\n",
      "tensor([[ 0.3473, -2.9985],\n",
      "        [-0.1728,  1.0055],\n",
      "        [-0.1656,  0.9867],\n",
      "        [ 1.2761, -0.1070],\n",
      "        [ 0.5659, -0.5252]])\n",
      "hidden fc1\n",
      "[5, 4]\n",
      "tensor([[ 0.9088,  0.2307, -0.3162,  0.1287],\n",
      "        [-0.0486, -0.8633,  0.4945, -0.9935],\n",
      "        [-0.0453, -0.8562,  0.4880, -0.9866],\n",
      "        [-0.1144, -0.0165, -0.4570, -0.2470],\n",
      "        [ 0.1801, -0.2204, -0.1160, -0.3846]], grad_fn=<AddmmBackward>)\n",
      "drop_output\n",
      "[5, 4]\n",
      "tensor([[ 0.0000,  0.0000, -0.3952,  0.1609],\n",
      "        [-0.0607, -1.0792,  0.6181, -0.0000],\n",
      "        [-0.0000, -1.0703,  0.6100, -1.2333],\n",
      "        [-0.1430, -0.0000, -0.5712, -0.0000],\n",
      "        [ 0.0000, -0.0000, -0.1450, -0.4807]], grad_fn=<MulBackward0>)\n",
      "relu\n",
      "[5, 4]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1609],\n",
      "        [0.0000, 0.0000, 0.6181, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "output fc2\n",
      "[5, 1]\n",
      "tensor([[0.3074],\n",
      "        [0.5325],\n",
      "        [0.5290],\n",
      "        [0.2628],\n",
      "        [0.2628]], grad_fn=<AddmmBackward>)\n",
      "sigmoid_output\n",
      "[5, 1]\n",
      "tensor([[0.5763],\n",
      "        [0.6301],\n",
      "        [0.6292],\n",
      "        [0.5653],\n",
      "        [0.5653]], grad_fn=<SigmoidBackward>)\n",
      "Epoch 0: train loss: 0.6900337934494019\n",
      "Epoch 1: train loss: 0.6602381467819214\n",
      "Epoch 2: train loss: 0.6870923638343811\n",
      "Epoch 3: train loss: 0.6639739871025085\n",
      "Epoch 4: train loss: 0.6590121388435364\n",
      "Epoch 5: train loss: 0.6788594126701355\n",
      "Epoch 6: train loss: 0.6777434349060059\n",
      "Epoch 7: train loss: 0.6546610593795776\n",
      "Epoch 8: train loss: 0.6759697198867798\n",
      "Epoch 9: train loss: 0.6526777148246765\n",
      "Epoch 10: train loss: 0.6516193151473999\n",
      "Epoch 11: train loss: 0.6766926646232605\n",
      "Epoch 12: train loss: 0.6759742498397827\n",
      "Epoch 13: train loss: 0.6723896861076355\n",
      "Epoch 14: train loss: 0.6421281099319458\n",
      "Epoch 15: train loss: 0.6943062543869019\n",
      "Epoch 16: train loss: 0.7005182504653931\n",
      "Epoch 17: train loss: 0.6461120843887329\n",
      "Epoch 18: train loss: 0.6424296498298645\n",
      "Epoch 19: train loss: 0.6680032014846802\n"
     ]
    }
   ],
   "source": [
    "dump_ = 1\n",
    "model = Feedforward(2, 4,0.2)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "print(model)\n",
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):    \n",
    "    optimizer.zero_grad()    \n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)    \n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#input        [5, 2]  our input shape\n",
    "#hidden fc1   [5, 4]  (fc1): Linear(in_features=2, out_features=4, bias=True) \n",
    "#drop_output  [5, 4]  since =0.1 nonzero outputs are multiplied by 1.25\n",
    "#relu         [5, 4]  relu same size\n",
    "#output fc2   [5, 1]  Linear(in_features=4, out_features=1, bias=True)\n",
    "#sigmoid_output  [5, 1]  Sigmoid\n",
    "#*** check the dropout values, they are either zero or scaled by 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "[5, 2]\n",
      "tensor([[ 0.2435,  1.1330],\n",
      "        [-1.1941, -0.3866],\n",
      "        [ 1.5834,  0.1661],\n",
      "        [-1.2566, -0.0566],\n",
      "        [-1.0901, -1.0576]])\n",
      "hidden fc1\n",
      "[5, 4]\n",
      "tensor([[-0.1966, -0.7121,  0.3068, -0.8723],\n",
      "        [ 0.6004, -0.9878,  0.8927, -1.0135],\n",
      "        [-0.2531,  0.0488, -0.5807, -0.2151],\n",
      "        [ 0.5210, -1.0835,  0.9749, -1.1070],\n",
      "        [ 0.7676, -0.8027,  0.7384, -0.8310]], grad_fn=<AddmmBackward>)\n",
      "drop_output\n",
      "[5, 4]\n",
      "tensor([[-0.1966, -0.7121,  0.3068, -0.8723],\n",
      "        [ 0.6004, -0.9878,  0.8927, -1.0135],\n",
      "        [-0.2531,  0.0488, -0.5807, -0.2151],\n",
      "        [ 0.5210, -1.0835,  0.9749, -1.1070],\n",
      "        [ 0.7676, -0.8027,  0.7384, -0.8310]], grad_fn=<AddmmBackward>)\n",
      "relu\n",
      "[5, 4]\n",
      "tensor([[0.0000, 0.0000, 0.3068, 0.0000],\n",
      "        [0.6004, 0.0000, 0.8927, 0.0000],\n",
      "        [0.0000, 0.0488, 0.0000, 0.0000],\n",
      "        [0.5210, 0.0000, 0.9749, 0.0000],\n",
      "        [0.7676, 0.0000, 0.7384, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "output fc2\n",
      "[5, 1]\n",
      "tensor([[0.3666],\n",
      "        [0.4857],\n",
      "        [0.2341],\n",
      "        [0.5420],\n",
      "        [0.3756]], grad_fn=<AddmmBackward>)\n",
      "sigmoid_output\n",
      "[5, 1]\n",
      "tensor([[0.5906],\n",
      "        [0.6191],\n",
      "        [0.5583],\n",
      "        [0.6323],\n",
      "        [0.5928]], grad_fn=<SigmoidBackward>)\n",
      "Test loss before training 0.794707179069519\n"
     ]
    }
   ],
   "source": [
    "dump_ = 1\n",
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())\n",
    "#since we are at eval mode, dropout does not have any effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
