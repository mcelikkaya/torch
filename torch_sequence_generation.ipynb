{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_sequence_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "goxkU5ztfF9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "59525372-9a91-48b6-cf07-c4f85f1dc125"
      },
      "source": [
        "\"\"\"\n",
        "In this notebook , I try m -> n sequence generation.\n",
        "my alphabet has 5 letters \"abcde\"\n",
        "I create a sequence of 6 letters and output most occuring 2 letters.\n",
        "\n",
        "input  = ['a', 'b', 'a', 'd', 'b', 'a']\n",
        "output = ['a', 'b']\n",
        "\n",
        "1st is most occuring letter\n",
        "2ns is second most occuring.\n",
        "In eval method  i both measure, any match of sequence\n",
        "and full match of sequence.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "from io import open\n",
        "import os, string, random, time, math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe647f5ee50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73PRdjE8fHgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#My utility to log intermediate steps\n",
        "class StepLogger():\n",
        "    def __init__(self,capacity):\n",
        "        self.tensor_datas = {}        \n",
        "        self.capacity = capacity\n",
        "        self.added_labels = []\n",
        "        \n",
        "    \n",
        "    def add_info(self,tensor_data,tensor_label):\n",
        "        if tensor_label not in self.added_labels:\n",
        "            self.added_labels.append( tensor_label )\n",
        "        \n",
        "        if tensor_label in self.tensor_datas.keys():\n",
        "            current_arr = self.tensor_datas.get(tensor_label)\n",
        "            if len(current_arr) < self.capacity:\n",
        "                current_arr = self.tensor_datas.get(tensor_label, [])\n",
        "                current_arr.append(tensor_data)\n",
        "        else:\n",
        "            self.tensor_datas[tensor_label] = [tensor_data]\n",
        "    \n",
        "    def get_default_summary(self,show_data=False,summary_count=1):\n",
        "        self.get_summary(self.added_labels,show_data)\n",
        "        \n",
        "    def get_summary(self,labels,show_data=False,summary_count=1):\n",
        "        count = 0\n",
        "        for i in range(summary_count):\n",
        "            for l in labels:\n",
        "                label_data = self.tensor_datas.get(l)[count]\n",
        "                print(l)\n",
        "                if torch.is_tensor(label_data):\n",
        "                    print( list(label_data.size() ) )\n",
        "                if not show_data and not torch.is_tensor(label_data):\n",
        "                    print(label_data)\n",
        "                if show_data:    \n",
        "                    print(label_data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJoHHM-3fKUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "142dbb7c-5176-4271-fe48-c1d8c5fabf0e"
      },
      "source": [
        "#data generation methods\n",
        "all_letters = \"abcde\"\n",
        "\n",
        "\n",
        "def prepare_sequence(name):\n",
        "    rep = np.zeros( (len(name), 1) )\n",
        "    for index, letter in enumerate(name):\n",
        "        pos = all_letters.find(letter)\n",
        "        rep[index][0] = pos\n",
        "    return rep\n",
        "\n",
        "def gen_1_tuple():\n",
        "    seq_len = 3\n",
        "    template = [1,1,2,2,3,3,4,4,5,5]\n",
        "    sequence = random.sample(template,3)\n",
        "    b = {}\n",
        "    for item in sequence:\n",
        "        #only count smaller than 10\n",
        "        if item < 10:\n",
        "            b[item] = b.get(item, 0) + 1    \n",
        "    sb = sorted(b.items(), key=lambda x: x[1],reverse=True)   \n",
        "    most_occur = 1\n",
        "    most_occur_2 = 1\n",
        "    try:\n",
        "        most_occur   = sb[0][0]\n",
        "        \n",
        "        most_occur_2 = sb[1][0]\n",
        "    except Exception as ex:\n",
        "        print(\"sb \",b)\n",
        "        most_occur = 1\n",
        "        \n",
        "            \n",
        "    result_list = sequence+[most_occur,most_occur,most_occur_2] \n",
        "    result_list = random.sample(result_list,len(result_list))\n",
        "    to_word = [all_letters[i-1] for i in result_list ]\n",
        "    return to_word,[all_letters[most_occur-1],all_letters[most_occur_2-1] ]\n",
        "\n",
        "def nat_rep(data_index):\n",
        "    return np.array([all_letters.index(i)for i in data_index])\n",
        "\n",
        "def gen_data_tuple():\n",
        "    d1 = gen_1_tuple()\n",
        "    return d1    \n",
        "    \n",
        "d = gen_data_tuple()\n",
        "print(d)\n",
        "nat_rep(d[1])\n",
        "#seq_rep(d[0])\n",
        "db = gen_data_tuple()\n",
        "print(db)\n",
        "print(len(db[0]))\n",
        "print(nat_rep(db[1]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['c', 'a', 'a', 'c', 'a', 'b'], ['a', 'c'])\n",
            "(['a', 'e', 'a', 'a', 'e', 'b'], ['a', 'e'])\n",
            "6\n",
            "[0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHs582bjfaA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trains = [ gen_data_tuple() for _ in range(2000)]\n",
        "X_train = [t[0] for t in trains ]\n",
        "y_train = [t[1] for t in trains ]\n",
        "\n",
        "tests = [ gen_data_tuple() for _ in range(200)]\n",
        "X_test = [t[0] for t in tests ]\n",
        "y_test = [t[1] for t in tests ]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EE1fJbpffnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "c68d4e07-0073-498f-fed4-14dd52ec5f37"
      },
      "source": [
        "print(\"data \" ,\" \".join(X_train[0]) )\n",
        "print(\"label \" ,y_train[0] )\n",
        "\n",
        "print(\"data \" ,prepare_sequence(X_train[0]) )\n",
        "print(\"label \" ,prepare_sequence(y_train[0]) )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  b c c d b b\n",
            "label  ['b', 'c']\n",
            "data  [[1.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [1.]\n",
            " [1.]]\n",
            "label  [[1.]\n",
            " [2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuzPdB6Tfi4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "f9f2e57d-f2f4-4821-a603-e395d6e5405d"
      },
      "source": [
        "#check distribution of data\n",
        "count = {}\n",
        "for l in all_letters:\n",
        "    count[l] = 0\n",
        "\n",
        "for d in trains:\n",
        "    count[d[1][0]] += 1\n",
        "    count[d[1][1]] += 1\n",
        "\n",
        "#plot the distribution\n",
        "plt.style.use(\"seaborn\")\n",
        "plt_ = sns.barplot(list(count.keys()), list(count.values()))\n",
        "plt_.set_xticklabels(plt_.get_xticklabels(), rotation = 90)\n",
        "plt.show()   "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgwMaNPUfvt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)        \n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size * 2)\n",
        "        \n",
        "        #self.softAct = nn.Softmax(dim=1)\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        step_logger.add_info(sentence,\"forward sentence\")\n",
        "        \n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        step_logger.add_info(embeds,\"forward embeds\")\n",
        "        \n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        step_logger.add_info(lstm_out,\"forward lstm_out\")\n",
        "                \n",
        "        last_layer =  lstm_out[-1]\n",
        "        step_logger.add_info(last_layer,\"forward last_layer\")\n",
        "\n",
        "        tag_space =  self.hidden2tag(last_layer)\n",
        "        step_logger.add_info(tag_space,\"forward tag_space\")\n",
        "\n",
        "        tag_view = tag_space.view(2, -1)\n",
        "        step_logger.add_info(tag_view,\"forward tag_view\")        \n",
        "        \n",
        "        tag_scores = F.log_softmax(tag_view)\n",
        "        step_logger.add_info(tag_scores,\"forward tag_scores\")\n",
        "        \n",
        "        return tag_scores"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOG3tcxigJuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "def eval(model_,   X_, y_,full_match):\n",
        "     \n",
        "     correct = 0\n",
        "     #model_.eval()\n",
        "     \n",
        "     for index in range(len(X_)):\n",
        "         test_input = X_[index]\n",
        "         test_output = y_[index]\n",
        "         \n",
        "         actuals,expecteds,score = infer( model_, test_input,test_output,full_match)\n",
        "         correct += score\n",
        "         \n",
        "     #print(\"  correct\",correct,\"len(X_) \",len(X_))       \n",
        "     accuracy = 0                   \n",
        "     if full_match : \n",
        "       accuracy = (correct * 100 ) /len(X_)\n",
        "     else:  \n",
        "       accuracy = ( correct * 100 ) / ( len(X_) * 2) #we are counting partial match as partial success\n",
        "     return accuracy\n",
        "\n",
        "  \n",
        "\n",
        "def infer(model_,test_input,test_output,full_match):\n",
        "  \n",
        "  name_ohe = prepare_sequence(test_input)\n",
        "  \n",
        "  t = torch.tensor(name_ohe, dtype = torch.long)\n",
        "\n",
        "  tag_scores = model_(t)  \n",
        "  val, indices = tag_scores.topk(1)\n",
        "\n",
        "  actuals = indices.squeeze().numpy()\n",
        "  expecteds = prepare_sequence(test_output).flatten().astype(int)\n",
        "  #print(\"actuals \",actuals,\" expecteds \",expecteds)\n",
        "  score =0\n",
        "  if full_match :\n",
        "    score = int( actuals[0] == expecteds[0] and actuals[1] == expecteds[1])\n",
        "  else :\n",
        "    score = int(actuals[0] == expecteds[0]) + int(actuals[1] == expecteds[1])\n",
        "\n",
        "  return actuals,expecteds,score\n",
        "\n",
        "def train_epoch(shuffled,optimizer):\n",
        "  last_loss = 0\n",
        "  for sentence, tags in shuffled:\n",
        "    # Step 1. Remember that Pytorch accumulates gradients.\n",
        "    # We need to clear them out before each instance\n",
        "    model.zero_grad()\n",
        "    #print(\"pass\")\n",
        "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "    # Tensors of word indices.\n",
        "    #print(\"tags\",tags)\n",
        "    \n",
        "    sentence_in = torch.tensor(prepare_sequence(sentence), dtype=torch.long) \n",
        "    targets = torch.tensor(prepare_sequence(tags), dtype=torch.long )  \n",
        "            \n",
        "    step_logger.add_info(sentence_in,\"train sentence_in\")\n",
        "\n",
        "    # Step 3. Run our forward pass.\n",
        "    tag_scores = model(sentence_in)\n",
        "\n",
        "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "    #  calling optimizer.step()\n",
        "    \n",
        "    step_logger.add_info(tag_scores.squeeze(),\"train tag_scores\")\n",
        "    step_logger.add_info(targets,\"train targets\")\n",
        "    \n",
        "    loss = loss_function(tag_scores.squeeze(), targets.squeeze())\n",
        "    loss.backward()\n",
        "    optimizer.step()  \n",
        "    last_loss = loss\n",
        "  return last_loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry9MJ_JAgNgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#definition of model\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, 5, 5)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "step_logger =  StepLogger(2)\n",
        "training_data = zip(X_train,y_train)\n",
        "tranlist = list(training_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QqsCS9XgSRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "72c5a042-2d5c-441b-a2e0-ac4ec012156a"
      },
      "source": [
        "import timeit\n",
        "\n",
        "\n",
        "for epoch in range(21): \n",
        "    start_time = timeit.default_timer() # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    #print(\"\\n\\n\",\"epoch\",epoch)\n",
        "    if epoch % 2 == 1:\n",
        "      #tests = [ gen_data_tuple() for _ in range(200)]\n",
        "      with torch.no_grad() :\n",
        "        #clear_output(wait = True)\n",
        "        print(\"--------------------\")\n",
        "        print(epoch ,\"   Epoch Full sequence \", round(eval(model,  X_test, y_test,True),2) )\n",
        "        print(epoch ,\"   Epoch Partial       \" ,round(eval(model,  X_test, y_test,False),2) )\n",
        "\n",
        "    \n",
        "    shuffled = tranlist #random.sample(tranlist, len(tranlist))  \n",
        "     \n",
        "    #model.train()\n",
        "    #with torch.grad():\n",
        "    epoch_loss = train_epoch(shuffled,optimizer)    \n",
        "    elapsed = timeit.default_timer() - start_time    \n",
        "    #print(epoch,\" elapsed \",elapsed,\" epoch_loss \",epoch_loss)\n",
        "    \n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "1    Epoch Full sequence  54.5\n",
            "1    Epoch Partial        76.75\n",
            "--------------------\n",
            "3    Epoch Full sequence  79.0\n",
            "3    Epoch Partial        87.5\n",
            "--------------------\n",
            "5    Epoch Full sequence  88.5\n",
            "5    Epoch Partial        93.5\n",
            "--------------------\n",
            "7    Epoch Full sequence  85.0\n",
            "7    Epoch Partial        90.0\n",
            "--------------------\n",
            "9    Epoch Full sequence  91.5\n",
            "9    Epoch Partial        95.25\n",
            "--------------------\n",
            "11    Epoch Full sequence  85.0\n",
            "11    Epoch Partial        91.0\n",
            "--------------------\n",
            "13    Epoch Full sequence  92.0\n",
            "13    Epoch Partial        96.0\n",
            "--------------------\n",
            "15    Epoch Full sequence  92.5\n",
            "15    Epoch Partial        96.0\n",
            "--------------------\n",
            "17    Epoch Full sequence  93.5\n",
            "17    Epoch Partial        96.5\n",
            "--------------------\n",
            "19    Epoch Full sequence  92.5\n",
            "19    Epoch Partial        95.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bKx6MpSgUgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "84b2414b-46b7-4169-81f5-ca8fd70f95d2"
      },
      "source": [
        "#check intermediate object shapes\n",
        "step_logger.get_default_summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train sentence_in\n",
            "[6, 1]\n",
            "forward sentence\n",
            "[6, 1]\n",
            "forward embeds\n",
            "[6, 1, 6]\n",
            "forward lstm_out\n",
            "[6, 1, 6]\n",
            "forward last_layer\n",
            "[1, 6]\n",
            "forward tag_space\n",
            "[1, 10]\n",
            "forward tag_view\n",
            "[2, 5]\n",
            "forward tag_scores\n",
            "[2, 5]\n",
            "train tag_scores\n",
            "[2, 5]\n",
            "train targets\n",
            "[2, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_GtZ4zmibpD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f8f0ad7-deee-4055-8b0b-c80655e9389f"
      },
      "source": [
        "#check intermediate object samples\n",
        "step_logger.get_default_summary(True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train sentence_in\n",
            "[6, 1]\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [2],\n",
            "        [3],\n",
            "        [1],\n",
            "        [1]])\n",
            "forward sentence\n",
            "[6, 1]\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [2],\n",
            "        [3],\n",
            "        [1],\n",
            "        [1]])\n",
            "forward embeds\n",
            "[6, 1, 6]\n",
            "tensor([[[-0.9798, -1.6091, -0.7121,  0.3037, -0.7773, -0.2515]],\n",
            "\n",
            "        [[-0.2223,  1.6871, -0.3206, -0.2993,  1.8793, -0.0721]],\n",
            "\n",
            "        [[-0.2223,  1.6871, -0.3206, -0.2993,  1.8793, -0.0721]],\n",
            "\n",
            "        [[ 0.1578, -0.7735,  0.1991,  0.0457, -1.3924,  2.6891]],\n",
            "\n",
            "        [[-0.9798, -1.6091, -0.7121,  0.3037, -0.7773, -0.2515]],\n",
            "\n",
            "        [[-0.9798, -1.6091, -0.7121,  0.3037, -0.7773, -0.2515]]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "forward lstm_out\n",
            "[6, 1, 6]\n",
            "tensor([[[ 0.0621,  0.0146, -0.1223, -0.1063,  0.0445, -0.0541]],\n",
            "\n",
            "        [[-0.0070,  0.0163,  0.0202, -0.1210,  0.2521,  0.0947]],\n",
            "\n",
            "        [[-0.0153,  0.0292,  0.0216, -0.1029,  0.3384,  0.1647]],\n",
            "\n",
            "        [[-0.1313,  0.1670, -0.0013, -0.0857,  0.3372,  0.0374]],\n",
            "\n",
            "        [[-0.0218,  0.0408, -0.1264, -0.1433,  0.1905, -0.0391]],\n",
            "\n",
            "        [[ 0.0943,  0.0147, -0.1835, -0.1764,  0.1150, -0.0742]]],\n",
            "       grad_fn=<StackBackward>)\n",
            "forward last_layer\n",
            "[1, 6]\n",
            "tensor([[ 0.0943,  0.0147, -0.1835, -0.1764,  0.1150, -0.0742]],\n",
            "       grad_fn=<SelectBackward>)\n",
            "forward tag_space\n",
            "[1, 10]\n",
            "tensor([[-0.3619,  0.1778,  0.0149,  0.2526, -0.3037,  0.1353, -0.2485, -0.2635,\n",
            "          0.1491,  0.0740]], grad_fn=<AddmmBackward>)\n",
            "forward tag_view\n",
            "[2, 5]\n",
            "tensor([[-0.3619,  0.1778,  0.0149,  0.2526, -0.3037],\n",
            "        [ 0.1353, -0.2485, -0.2635,  0.1491,  0.0740]], grad_fn=<ViewBackward>)\n",
            "forward tag_scores\n",
            "[2, 5]\n",
            "tensor([[-1.9575, -1.4178, -1.5807, -1.3430, -1.8993],\n",
            "        [-1.4602, -1.8440, -1.8590, -1.4464, -1.5215]],\n",
            "       grad_fn=<LogSoftmaxBackward>)\n",
            "train tag_scores\n",
            "[2, 5]\n",
            "tensor([[-1.9575, -1.4178, -1.5807, -1.3430, -1.8993],\n",
            "        [-1.4602, -1.8440, -1.8590, -1.4464, -1.5215]],\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "train targets\n",
            "[2, 1]\n",
            "tensor([[1],\n",
            "        [2]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aUfNKididYm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "30055158-41a3-4a9b-ddc1-04bd316fcf0e"
      },
      "source": [
        "#test some samples\n",
        "for index_ in range(10):\n",
        "  print(X_test[index_],y_test[index_])\n",
        "  a,b,c = infer( model, X_test[index_],y_test[index_],False)\n",
        "  print( a,b,c)\n",
        "  a,b,c = infer( model, X_test[index_],y_test[index_],True)\n",
        "  print( a,b,c)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'b', 'b', 'a', 'c', 'b'] ['b', 'a']\n",
            "[1 0] [1 0] 2\n",
            "[1 0] [1 0] 1\n",
            "['a', 'b', 'b', 'a', 'e', 'a'] ['a', 'b']\n",
            "[0 1] [0 1] 2\n",
            "[0 1] [0 1] 1\n",
            "['b', 'b', 'd', 'b', 'b', 'd'] ['b', 'd']\n",
            "[1 3] [1 3] 2\n",
            "[1 3] [1 3] 1\n",
            "['e', 'd', 'd', 'e', 'd', 'd'] ['d', 'e']\n",
            "[3 4] [3 4] 2\n",
            "[3 4] [3 4] 1\n",
            "['a', 'e', 'd', 'e', 'a', 'a'] ['a', 'e']\n",
            "[0 4] [0 4] 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 4] [0 4] 1\n",
            "['a', 'c', 'a', 'c', 'c', 'e'] ['c', 'a']\n",
            "[2 0] [2 0] 2\n",
            "[2 0] [2 0] 1\n",
            "['d', 'c', 'd', 'e', 'c', 'd'] ['d', 'c']\n",
            "[3 2] [3 2] 2\n",
            "[3 2] [3 2] 1\n",
            "['d', 'd', 'b', 'b', 'b', 'b'] ['b', 'd']\n",
            "[1 1] [1 3] 1\n",
            "[1 1] [1 3] 0\n",
            "['b', 'c', 'b', 'b', 'c', 'b'] ['b', 'c']\n",
            "[1 2] [1 2] 2\n",
            "[1 2] [1 2] 1\n",
            "['a', 'a', 'a', 'e', 'd', 'd'] ['a', 'd']\n",
            "[0 3] [0 3] 2\n",
            "[0 3] [0 3] 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VF3Yzj-k23a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}